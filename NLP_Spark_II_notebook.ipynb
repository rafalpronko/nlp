{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/SparkML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "If you have any question about NLP or Machine Learning please ask me:\n",
    "1. Face To Face\n",
    "2. [On Piazza](http://piazza.com/nlprafalpronko/spring2018/nlp101) access code **nlp101**\n",
    "3. [On LinkedIn](https://www.linkedin.com/in/rafalpronko/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to put the item on Amazon in [English](http://amazon.com) to proper root category? \n",
    "\n",
    "![](img/BrowseNodes.png)\n",
    "\n",
    "Amazon browsenodes (categories) we can find here http://www.findbrowsenodes.com/. Please look at few first categories and their children - you can see the tree is quite big.\n",
    "\n",
    "Example of category path: 'Clothing, Shoes & Jewelry'->'Novelty, Costumes & More'->'Costumes & Accessories'->'More Accessories'->'Kids & Baby'\n",
    "\n",
    "to simplify our job we will try to assignee products (items) to root category. \n",
    "\n",
    "What's more, we want to use only **title** to achieve our goal. \n",
    "\n",
    "### How we want to achieve this goal? \n",
    "\n",
    "1. Scraping data from Amazon\n",
    "2. Analyzing collected data\n",
    "3. Building a model\n",
    "\n",
    "Initial data is provided by external company specialized in scraping sites.\n",
    "\n",
    "So it's the time to start!\n",
    "\n",
    "Next steps:\n",
    "1. We need choose only important data\n",
    "2. We need clean / normalize the text\n",
    "3. Building the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP move from Sklearn to SparkML\n",
    "\n",
    "\n",
    "\n",
    "Do you remember what we were doing at the morning? Here is small reminder:\n",
    "1. Read data to DataFrame\n",
    "2. Clear data / remove nulls / get only two columns\n",
    "3. Get root category from categories\n",
    "4. Split data to train test set\n",
    "5. Clear text data / remove stop words / lower case / normalize / build bag of words\n",
    "6. Labeled categories\n",
    "7. Build model / train model\n",
    "8. Evaluate model\n",
    "\n",
    "Now we will do the same but in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need start with Spark - create a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beacuse we have SparkContext load already we can use it and getOrCreate the spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "    .option(\"delimiter\", '^')\\\n",
    "    .csv('small_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.persist(pyspark.StorageLevel.MEMORY_AND_DISK) # if SPARK do not have enought memory - use this configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - show first 5 elements from DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show how many rows we loaded to our dataframe - in pandas we did it using .shape - in Spark we will do it using count().\n",
    "\n",
    "**Question**\n",
    "Do you remember what count() do in Pandas DataFrame? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas we had 240K do you know why here we have lower number?\n",
    "\n",
    "Do you remember: `option(\"mode\", \"DROPMALFORMED\")` - so if Spark cannot formatted the column in proper way it will just drop this columns (example we have a text which have some special character and Spark cannot escaped it in proper way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to see the number of nan / null value per columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.fillna(\"-1\").filter(\"title = -1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show null for salesRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have 9 null value in title. Above we showed the number of null / nan values only for single column below we will see how to do it for every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_data.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used isnan and isNull because we can have this two types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. Clear data / remove nulls / get only two columns\n",
    "3. Get root category from categories\n",
    "4. Split data to train test set\n",
    "5. Clear text data / remove stop words / lower case / normalize / build bag of words\n",
    "6. Labeled categories\n",
    "7. Build model / train model\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to remove null value? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we remove only 9 elements? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need only two columns from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.select('categories', 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 5 first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. Get root category from categries\n",
    "4. Split data to train test set\n",
    "5. Clear text data / remove stop words / lower case / normalize / build bag of words\n",
    "6. Labelized categories\n",
    "7. Build model / train model\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to extract root category - we can do it in the same way as in first notebook using `eval(str(x))[0][0]`.\n",
    "\n",
    "First we need to [define user function](https://gist.github.com/zoltanctoth/2deccd69e3d1cde1dd78). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf # import user definion function \n",
    "choose_only_first_root_category = udf(lambda x: eval(str(x))[0][0]) # create a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create new dataframe with additional column: \"root_category\". For this we will use [withColumn](https://docs.databricks.com/spark/latest/sparkr/functions/withColumn.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.withColumn('root_category', choose_only_first_root_category(df_data.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create a dataFram (name: df_data) with only two columns title and root_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. ~~Get root category from categries~~\n",
    "4. Split data to train test set\n",
    "5. Clear text data / remove stop words / lower case / normalize / build bag of words\n",
    "6. Labelized categories\n",
    "7. Build model / train model\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start splitting, to train test data we should check how many rows we have per category. We can do it by using [groupBy](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.groupBy('root_category').count().show(100, truncate=False) #truncate=False to show full name of category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see we have few categories with less than 1000 number of items, so we can remove this category from our dataframe.\n",
    "\n",
    "Why? Because we are almost sure that we cannot predict the true value for those categories (too small set of items). \n",
    "\n",
    "We can do it by using filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.filter('root_category != \"GPS & Navigation\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO remove all rows from categories with number of values below 1000 - additionaly remove category [ and empty\n",
    "# this categories are some errors during reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pyspark to split the data on to two groups - train / test we can use built-in function randomSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show how many rows we have in test and how many rows we have in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. ~~Get root category from categries~~\n",
    "4. ~~Split data to train test set~~\n",
    "5. Clear text data / remove stop words / lower case / normalize / build bag of words\n",
    "6. Labelized categories\n",
    "7. Build model / train model\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower\n",
    "\n",
    "As you remember the first step in data cleaning was lower case. In SPARK we need to import two function\n",
    "1. [lower](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.lower) - make lower text\n",
    "2. [col](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.col) - return the column based on the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train.withColumn('lower_sentence', lower(col('title')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "[Simple tokenizer](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer) similar as split() function in sklearn\n",
    "\n",
    "[Regex tokenizer](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer) tokenization based on regex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"lower_sentence\", outputCol=\"words_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = tokenizer.transform(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create tokenized word using regex tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question** \n",
    "\n",
    "what is the different between simple tokenization and regex tokenization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words_tokenizer\", outputCol=\"filtered\")\n",
    "train_clean = remover.transform(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer - Create bag of word model\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "model = cv.fit(train_clean)\n",
    "train_clean = model.transform(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember in first notebook we did something like lemmatization? But in Spark ml we do not have such thing. So for now we need to skip this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "To do the same cleaning for test set, remember that any model you have fit already so on test set you need to use only transform. \n",
    "\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = test.withColumn('lower_sentence', lower(col('title')))\n",
    "test_clean = tokenizer.transform(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 5 result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. ~~Get root category from categries~~\n",
    "4. ~~Split data to train test set~~\n",
    "5. ~~Clear text data / remove stop words / lower case / normalize / build bag of words~~\n",
    "6. Labelized categories\n",
    "7. Build model / train model\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String indexer\n",
    "\n",
    "As you remember in first model with sklearn we had to change the string category to index (labels) - in SPARK we have to do the same.\n",
    "\n",
    "In SPARK we have [StringIndexer](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"root_category\", outputCol=\"indexed\", handleInvalid='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_model = stringIndexer.fit(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = stringIndexer_model.transform(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show train_clean - only 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO do the same for test_clean - rememeber stringindexer you have trained so use only transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. ~~Get root category from categries~~\n",
    "4. ~~Split data to train test set~~\n",
    "5. ~~Clear text data / remove stop words / lower case / normalize / build bag of words~~\n",
    "6. ~~Labelized categories~~\n",
    "7. Build model / train model\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our favourite classifier - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes # import Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(modelType=\"multinomial\", featuresCol=\"features\", labelCol=\"indexed\",) # declare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.fit(train_clean) # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. ~~Get root category from categries~~\n",
    "4. ~~Split data to train test set~~\n",
    "5. ~~Clear text data / remove stop words / lower case / normalize / build bag of words~~\n",
    "6. ~~Labelized categories~~\n",
    "7. ~~Build model / train model~~\n",
    "8. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we need predict new value for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 5 rows from prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need import the model [evaluator](http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexed\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done / ToDo\n",
    "1. ~~Read data to DataFrame~~\n",
    "2. ~~Clear data / remove nulls / get only two columns~~\n",
    "3. ~~Get root category from categries~~\n",
    "4. ~~Split data to train test set~~\n",
    "5. ~~Clear text data / remove stop words / lower case / normalize / build bag of words~~\n",
    "6. ~~Labelized categories~~\n",
    "7. ~~Build model / train model~~\n",
    "8. ~~Evaluate model~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now you can create a simple model with simple count-vectorizer - split by the words (using unigrams). \n",
    "\n",
    "As you remember in first notebook we talk about ngrams (it was just mentioned) and this should improve our model. In Sklearn it was really simple - we need just change `ngrams_range=(1,1)` to for example `ngrams_range=(1,2)`\n",
    "\n",
    "In Spark it is more complicated. But now we will learn how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams model\n",
    "\n",
    "Simple explanation what are ngrams\n",
    "\n",
    "![](img/ngrams.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram # import ngram model from spark ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol='filtered', outputCol='ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngrams = ngram.transform(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngrams.select('ngrams').show(5, truncate=False) # truncate false allow us to show full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - create new ngrams column with ngram number = 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "name": "SPARK - nlp methods",
  "notebookId": 1898716600165204
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://spark.apache.org/images/spark-logo-trademark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After few weeks, scraping company prepared more data for us - previous we had only 240K rows of data, now we have few millions. Our laptops are not good enough for performing the analysis. We need to [find new solution](http://lmgtfy.com/?iie=1&q=fast+machine+learning+cluster+computing). \n",
    "\n",
    "The answer here is Apache Spark. Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apache Spark™ is a fast and general engine for large-scale data processing.\n",
    "\n",
    "\n",
    "> Apache Spark is a general-purpose cluster computing platform designed for faster processing. When it comes to speed, Spark extends the popular MapReduce model to efficiently support more types of computations, including interactive queries and stream processing. Spark is up to 10× faster than Hadoop for iterative applications, speeds up a real-world data analytics report by 40×, and can be used interactively to scan a 1 TB dataset with 5–7s latency.\n",
    "\n",
    "What will you learn from this notebook:\n",
    "1. ~~What is Spark~~\n",
    "2. How to create Spark session?\n",
    "3. How to create a data in Spark?\n",
    "4. Manipulate data in Spark\n",
    "5. How to read data to Spark using RDD?\n",
    "6. How create DataFrame?\n",
    "7. How to use DataFrame?\n",
    "8. How to use sql in DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a session in Spark - on our machine we have already SparkContext create but we need to check if its really exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ~~What is Spark~~\n",
    "2. ~~How to create Spark session?~~\n",
    "3. How to create a data in Spark?\n",
    "4. Manipulate data in Spark?\n",
    "5. How to read data to Spark using RDD?\n",
    "6. How create DataFrame?\n",
    "7. How to use DataFrame?\n",
    "8. How to use sql in DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first data object in Spark - we will create a simple list with four elements.\n",
    "\n",
    "RDD - Resilient Distributed Datasets are the core concepts in Spark. In order to understand how spark works, we should know what RDDs are and how they work. The Spark RDD is a fault tolerant, distributed collection of data that can be operated in parallel. Each RDD is split into multiple partitions, and spark runs one task for each partition. The Spark RDDs can contain any type of Python, Java or Scala objects, including user-defined classes. They are not actual data, but they are Objects, which contain information about data residing on the cluster. The RDDs try to solve these problems by enabling fault tolerant, distributed in-memory computations.\n",
    "\n",
    "\n",
    "The image below shows the iterative operation on Spark RDD - it will store intermediate result in distributed memory instead of disk. \n",
    "\n",
    "**Note** if we do not have enough RAM the result will be keep on disk. \n",
    "\n",
    "![](https://www.tutorialspoint.com/apache_spark/images/iterative_operations_on_spark_rdd.jpg)\n",
    "https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm\n",
    "\n",
    "Let's create our first RDD! \n",
    "\n",
    "To create RDD we will use [parallelize()](https://spark.apache.org/docs/latest/rdd-programming-guide.html#parallelized-collections) command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show n first elements we can use [take()](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the RDDs, you can use the distributed data in rdd to operate on in parallel. \n",
    "\n",
    "We have two types of operations: transformation and action. \n",
    "\n",
    "Transformations are lazy operations on a RDD that create one or many new RDDs, while actions produce non-RDD values: they return a result set, a number, a file,...\n",
    "\n",
    "Examples of transformations: `map()`, `filter()`, `flatMap()`, `sample()`,...\n",
    "Examples of action: `take()`, `first()`, `collect()`\n",
    "\n",
    "OK so let's show the differences between transformation and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it creates new RDD but not bring any result - lambda function add one two element from rdd\n",
    "rdd.map(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map is transformation and take is action\n",
    "rdd.map(lambda x: x + 1).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of course we can used multiple transformations and an action, ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 2 to any element in rdd and create new one\n",
    "rdd_new = rdd.map(lambda x: x + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only elements which is belowe 4\n",
    "rdd_new = rdd_new.filter(lambda x: x < 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we used `take` to show n first elements - now we will use [collect()](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.collect) to show all elements from rdd_new (remember try to not use\n",
    "collect in your application because this function send whole dispersed data to master node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_new.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ~~What is Spark~~\n",
    "2. ~~How to create Spark session?~~\n",
    "3. ~~How to create a data in Spark?~~\n",
    "4. ~~Manipulate data in Spark?~~\n",
    "5. How to read data to Spark using RDD?\n",
    "6. How create DataFrame?\n",
    "7. How to use DataFrame?\n",
    "8. How to use sql in DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data to RDD\n",
    "\n",
    "Classic example from hadoop tutorials - word counting\n",
    "\n",
    "We have a file with \"lorem\" text - let's count the words in this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need read the text file to rdd [textFile](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile('lorem.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the file text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Hello world' in Spark\n",
    "The 'Hello world' for Spark is a program which counting the words\n",
    "\n",
    "First of all, we need to split the lines by the words. For this we will use [flatMap](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.flatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = text_file.flatMap(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 5 first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word we need add the counter - constant 1 which create a tuple (word, 1)\n",
    "\n",
    "Example\n",
    "1. (word1, 1)\n",
    "2. (word2 , 1)\n",
    "3. (word1, 1)\n",
    "\n",
    "For this we will use the [map](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 5 first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference between map and flatMap!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.map(lambda line: line.split(' ')).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.flatMap(lambda line: line.split(' ')).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "than we need to summarize the the result with the same name\n",
    "\n",
    "Example:\n",
    "\n",
    "it takes first tuple (word1, 1) and will try to find the tuple with first element `word1` if it find we reduce this two tuple to one with `(word1, 2)` and so on\n",
    "\n",
    "For this we will use [reduceByKey()](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.reduceByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.reduceByKey(lambda a, b: a + b) # reduce by key word and add two value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO show 10 first result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simple sort the result descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also sort the result by value\n",
    "counts.sortBy(lambda x: x[1], False).take(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ~~What is Spark~~\n",
    "2. ~~How to create Spark session?~~\n",
    "3. ~~How to create a data in Spark?~~\n",
    "4. ~~Manipulate data in Spark?~~\n",
    "5. ~~How to read data to Spark using RDD?~~\n",
    "6. How create DataFrame?\n",
    "7. How to use DataFrame?\n",
    "8. How to use sql in DataFrame?\n",
    "\n",
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame is also available in Spark. It can be create from RDD - we just need rdd_name.toDF() but first we need to create sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DF from counts rdd\n",
    "df_count = counts.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ~~What is Spark~~\n",
    "2. ~~How to create Spark session?~~\n",
    "3. ~~How to create a data in Spark?~~\n",
    "4. ~~Manipulate data in Spark?~~\n",
    "5. ~~How to read data to Spark using RDD?~~\n",
    "6. ~~How create DataFrame?~~\n",
    "7. How to use DataFrame?\n",
    "8. How to use sql in DataFrame?\n",
    "\n",
    "#### Create DataFrame\n",
    "\n",
    "\n",
    "#### How to use DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas DataFrame we had a head function now we need to use [show()](http://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create the DataFrame with columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_with_col = counts.toDF(['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below I'll show you few really useful command on Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_with_col.count() #number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can select one columns\n",
    "df_counts_with_col.select('word').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can filter out rows - lets we say we want just rows where the word count is more than 6\n",
    "df_counts_with_col.filter(df_counts_with_col['count'] > 6).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the schema of df\n",
    "df_counts_with_col.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, count column has type \"long\" - let's change it to small int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * #import types from pyspark\n",
    "df_counts_with_col = df_counts_with_col.withColumn('count', df_counts_with_col['count'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check the changes (TIP show the schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add new column with length of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df_counts_with_col = df_counts_with_col.withColumn(\"word_length\", length(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_with_col.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change word_length to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum up \"count\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_with_col.rdd.map(lambda x: x['count']).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ~~What is Spark~~\n",
    "2. ~~How to create Spark session?~~\n",
    "3. ~~How to create a data in Spark?~~\n",
    "4. ~~Manipulate data in Spark?~~\n",
    "5. ~~How to read data to Spark using RDD?~~\n",
    "6. ~~How create DataFrame?~~\n",
    "7. ~~How to use DataFrame?~~\n",
    "8. How to use sql in DataFrame?\n",
    "\n",
    "#### How to use sql in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If someone prefer to use the sql instead of pyspark function we can do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_with_col.createOrReplaceTempView(\"count\") # register temporary SQL view table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"select * from count limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - use SQL to find the sum of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "pySparki python3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "name": "Spark introduction",
  "notebookId": 1898716600165144,
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/wordcloud.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook - cheatsheet\n",
    "\n",
    "Few useful command for notebook:\n",
    "1. shift+enter - run command in cell\n",
    "2. Enter - open edit mode for cell\n",
    "\n",
    "Try Enter on this cell\n",
    "Try shift + enter on this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question\n",
    "If you have any question about NLP or Machine Learning please ask me:\n",
    "1. Face To Face\n",
    "2. [On Piazza](http://piazza.com/nlprafalpronko/spring2018/nlp101) access code **nlp101**\n",
    "3. [On LinkedIn](https://www.linkedin.com/in/rafalpronko/)\n",
    "\n",
    "If you want practice in home I created a dockerFile with python+spark it can be downloaded from [here](https://github.com/rafalpronko/python-pyspark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================\n",
    "# Let's create new start-up: Global Listings\n",
    "\n",
    "\n",
    ">Our goal is to fully democratize international e-Commerce,\n",
    "making it possible for any company, small or large, to offer their products worldwide\n",
    "while providing superior customer service.\n",
    "\n",
    "one branch of activity is translation the auction between eBay and Amazon marketplaces. For this purpose you need to tackle following challenges:\n",
    "1. How to choose proper category for the products on market place? - **our main problem**\n",
    "2. How to enrich the product from Amazon to Ebay?\n",
    "3. How to prepare shorter title during conversion from Amazon to Ebay?\n",
    "4. How to create a bullet points from Ebay to Amazon?\n",
    "5. Machine translation problem\n",
    "\n",
    "Because our client can sell / publish only well categorized items we need to solve the first problem ASAP. \n",
    "\n",
    "First what you need to do is... [research](http://lmgtfy.com/?q=e-commerce+items+categorization) the state-of-the-art solution. \n",
    "\n",
    "\n",
    "After a week research you will find few nice solutions:\n",
    "1. [Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks](http://www.kdd.org/kdd2016/subtopic/view/large-scale-item-categorization-in-e-commerce-using-multiple-recurrent-neur/)\n",
    "2. [Large-scale Multi-class and Hierarchical Product Categorization for an\n",
    "E-commerce Giant](https://www.aclweb.org/anthology/C/C16/C16-1051.pdf)\n",
    "3. [Classifying e-commerce products based on images and text](http://cbonnett.github.io/Insight.html)\n",
    "\n",
    "Now you know - everything you need is **Natural Language Processing** and **text mining**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing / Text mining - short introduction\n",
    "\n",
    ">Natural Language Processing (NLP) is “ability of machines to understand and interpret human language the way it is written or spoken.”\n",
    "\n",
    "## Key Application Areas of Natural Language Processing\n",
    "\n",
    "### Automatic summarizer\n",
    "For given text we need create a summary of text / extract key-point.\n",
    "### Sentimental analysis\n",
    "For given text we need to predict the subject.\n",
    "### <span style=\"color:red\">Text classification</span> - our problem ;)\n",
    "It is designed to categorize different journals, news stories according to their domain. Multi-document classification is also possible. The most popular example of text classification is spam detection in emails.\n",
    "### Information extraction\n",
    "Extract the key information from the text e.g. date from email - to create calendar events. \n",
    "\n",
    "![](img/future-applications-of-nlp.png) \n",
    "source: https://www.xenonstack.com/blog/data-science/overview-of-artificial-intelligence-and-role-of-natural-language-processing-in-big-data\n",
    "\n",
    "In this short introduction to Natural Language Processing (NLP) we will cover topics like:\n",
    "1. Read data to Pandas dataframe\n",
    "2. Fill null data, remove null rows, select interesting rows from dataframe\n",
    "3. Clean data: remove stopwords, remove noise, lemmatization, stemmining, word tokenization, ngrams generation\n",
    "4. Vector representation for text: bag of word, tf-idf\n",
    "5. Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to put the item on Amazon in [English](http://amazon.com) to proper root category? \n",
    "\n",
    "![](img/BrowseNodes.png)\n",
    "\n",
    "Amazon browsenodes (categories) we can find here http://www.findbrowsenodes.com/. Please look at few first categories and their children - you can see the tree is quite big.\n",
    "\n",
    "Example of category path: 'Clothing, Shoes & Jewelry'->'Novelty, Costumes & More'->'Costumes & Accessories'->'More Accessories'->'Kids & Baby'\n",
    "\n",
    "to simplify our job we will try to assignee products (items) to root category. \n",
    "\n",
    "What's more, we want to use only **title** to achieve our goal. \n",
    "\n",
    "### How we want to achieve this goal? \n",
    "\n",
    "1. Scraping data from Amazon\n",
    "2. Analyzing collected data\n",
    "3. Building a model\n",
    "\n",
    "Initial data is provided by external company specialized in scraping sites.\n",
    "\n",
    "So it's the time to start!\n",
    "\n",
    "Next steps:\n",
    "1. We need choose only important data\n",
    "2. We need clean / normalize the text\n",
    "3. Building the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and manipalate data  - Pandas DataFrame\n",
    "Data frame is a way to store data in rectangular grids that can easily be overviewed. You can imagine the data frame as table in database where you have a rows and columns. On this workshop we will used dataframe to storage data from file (in bare python and in pyspark). In Python we have nice library for dataframe: Pandas. \n",
    "\n",
    "\n",
    "More information about DataFrame and Pandas: https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start working with Pandas we need import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # if we need use some library in python we need to import this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__ # we can check the version of our library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data to Pandas\n",
    "\n",
    "To Pandas we can load many types of data sources: \n",
    "1. Excel\n",
    "2. CSV\n",
    "3. SQL\n",
    "4. Google Big Query,\n",
    "...\n",
    "On this workshop we will use csv file\n",
    "\n",
    "To read data from csv to pandas we use [read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('small_data.csv', sep=\"^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see our data in dataframe we can use few functions - the most popular one is [head()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html) - to show n first elements from dataframe (we have also tail function - to show n last elements).\n",
    "\n",
    "[More information](https://pandas.pydata.org/pandas-docs/stable/basics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns # show names of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data description:\n",
    "* **asin** - unique identifier\n",
    "* **salesRank** - seller's rank in particular category\n",
    "* **ImUrl** - image url\n",
    "* **categories** - categories of the product\n",
    "* **title** - title of the product\n",
    "* **description** - description\n",
    "* **price** - price\n",
    "* **related** - which product is related to this one\n",
    "* **brand** - product brand\n",
    "\n",
    "Now we should look closer to the data. Let's sum up each columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many rows and columns we have in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see not empty values devide by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count can show us how many empty value we have - we know in each column should be 240000 rows - if count show us less it means we have some empty value - we need to tackle this issue. \n",
    "\n",
    "Beacouse we need only two columns - categories and title - lets focuse only on this. \n",
    "\n",
    "Categories has 240000 - so zero empty value but title - 239991 - number of empty value is real small (240000 - 239991) so we can simple remove this rows. \n",
    "\n",
    "For more information how to work with empty value you can find [here](https://pandas.pydata.org/pandas-docs/stable/missing_data.html)\n",
    "\n",
    "**Remove** \n",
    "\n",
    "In pandas we have a simple method to remove empty values. Just we need to use `.dropna(subset, inplace=True/False)` on dataframe.\n",
    "\n",
    "subset is a list of column where the dataframe will be looking for empyt value\n",
    "inplace=True - it means the operation should be done on this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dropna(subset=['title'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - show how many items left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our exercise we will need only two columns `title` and `categories` so let's we remove other columns - for this we will use [drop(subset, axis=1/0,inplace=True)](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) function.\n",
    "\n",
    "subset / inplace - the same like in dropna()\n",
    "axis=1 - means remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.drop(['asin', 'salesRank', 'imUrl', 'description', 'price', 'related', 'brand'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `inplace=True` because we want to change current dataFrame, `axis=1` - drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - show how many rows and columns is still in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the columns in our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing null values and keeping only two columns, we can look at our target column (categories). \n",
    "\n",
    "As we saw above we can show only limited number of characters in columns as default, fortunately we can change it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show first 10 line (you can do it by .head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. How many categories has first/second/third row? \n",
    "2. What you can observe? What is your first impression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have mixed type of categories: \n",
    "* just root category (root category is the first one in whole tree)\n",
    "* root category and its children\n",
    "* items belong to more then one category tree (3rd row).\n",
    "\n",
    "To simplify our job we just will try to build the classifier for root category. \n",
    "\n",
    "We need to extract root category from our structure. \n",
    "\n",
    "This structure ```[['Clothing, Shoes & Jewelry', 'Girls'], ['Clothing, Shoes & Jewelry', 'Novelty, Costumes & More', 'Costumes & Accessories', 'More Accessories', 'Kids & Baby']]``` is called list of list - something between ```[]``` is just a list - https://docs.python.org/2/tutorial/datastructures.html. \n",
    "\n",
    "But in dataframe cell this is just a text - we need evaluate this string to the list (https://docs.python.org/2/library/functions.html#eval).\n",
    "\n",
    "And we need to apply this eval function for all rows - in pandas we can do it via apply (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_only_first_root_category(element):\n",
    "    return eval(element)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific case eval means - change string `\"[['Clothing, Shoes & Jewelry', 'Girls'],...\"` to list `[['Clothing, Shoes & Jewelry', 'Girls'],...` and take only first element from first sublist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['categories'] = df_data.categories.apply(choose_only_first_root_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show the 15 first elements from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to build our classifier\n",
    "\n",
    "1. ~~We need to choose only interesting data~~\n",
    "2. We need clean / normalize the text\n",
    "3. Build the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the text\n",
    "\n",
    "In our classifier we want to use a simple method representation of text data - **bag of word**. \n",
    "\n",
    "###  How Bag of Word works. \n",
    "\n",
    "As a example let's take two sentences: ```The quick brown fox jumps over the lazy dog``` and ```Never jump over the lazy dog quickly```. In our example this two sentences is called corpus. For this corpus we need to create a vocabulary:\n",
    "```\n",
    "{\n",
    "The: 1,\n",
    "quick: 2,\n",
    "brown: 3,\n",
    "fox: 4,\n",
    "jumps: 5,\n",
    "over: 6,\n",
    "the: 7, \n",
    "lazy: 8,\n",
    "dog: 9,\n",
    "Never: 10,\n",
    "jump: 11,\n",
    "quickly: 12\n",
    "}\n",
    "```\n",
    "than we create a vector over each sentence. \n",
    "\n",
    "First sentence:\n",
    "\n",
    "```[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]```\n",
    "\n",
    "```[The, quick, brown, fox, jumps, over, the, lazy, dog, , , ]```\n",
    "\n",
    "second\n",
    "\n",
    "```[0, 0, 0, 0, 0,  1, 1, 1, 1, 1, 1, 1]```\n",
    "\n",
    "```[ , , , , , over, the, lazy, dog, Never, jump, quickly]```\n",
    "\n",
    "What is not important in this representation:\n",
    "1. order\n",
    "2. capital letters\n",
    "3. `quickly` can be equal to `quick`,... \n",
    "4. `the` is just a stop word (not informative)\n",
    "\n",
    "so our vocabulary should looks like:\n",
    "\n",
    "```\n",
    "{\n",
    "quick: 1,\n",
    "brown: 2,\n",
    "fox: 3,\n",
    "jump: 4,\n",
    "over: 5,\n",
    "lazy: 6,\n",
    "dog: 7,\n",
    "never: 8,\n",
    "}\n",
    "```\n",
    "Try to think how our vectors should looks like...\n",
    "\n",
    "\n",
    "To summarize observation and clean / normalize text we need to:\n",
    "\n",
    "1. Make sentences lowercase\n",
    "2. Split it by the words\n",
    "3. Remove stop words\n",
    "4. cast quickly to quick, jumps to jump and so on\n",
    "\n",
    "Let's start clearing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sentece\n",
    "sentence = \"The quick brown fox jumps over the lazy do\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting by the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_split = sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can use the [nltk](http://www.nltk.org/) library and word tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize # import ready function from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenize = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference? \n",
    "Let's take another example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Two books, in my shop\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"Two books, in my shop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in first case we have `book,` in second `book` and `,` are split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text: \n",
    "1. ~~Make sentences lowercase~~\n",
    "2. ~~Split it by the words~~\n",
    "3. Remove stop words\n",
    "4. cast quickly to quick, jumps to jump and so on\n",
    "\n",
    "Now time to remove stop words - for this we can use nltk and [stop words](https://en.wikipedia.org/wiki/Stop_words) build in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words language - our titles are in English\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sentence_tokenize:\n",
    "    if word not in stop_words:\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text: \n",
    "1. ~~Make sentences lowercase~~\n",
    "2. ~~Split it by the words~~\n",
    "3. ~~Remove stop words~~\n",
    "4. cast quickly to quick, jumps to jump and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast quickly to quick\n",
    "**[Lammanize](https://en.wikipedia.org/wiki/Lemmatisation) and [stemming](https://en.wikipedia.org/wiki/Stemming)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer # we need import lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer() # create a lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lem.lemmatize('jumps')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer # import stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()\n",
    "print(stem.stem('jumps')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lem.lemmatize('quickly')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stem.stem('quickly')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text: \n",
    "1. ~~Make sentences lowercase~~\n",
    "2. ~~Split it by the words~~\n",
    "3. ~~Remove stop words~~\n",
    "4. ~~cast quickly to quick, jumps to jump and so on~~\n",
    "\n",
    "Ok, we've finished cleaning the single sentence - now is time to create a dictionary and vectors. Quite a lot to do but we can use build-in function for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster method to clean, normalize and build the bag of word\n",
    "\n",
    "For this tasks we have a  library in Python - sklearn. \n",
    "\n",
    "Ok, let's return to our main goal and our main dataset (as a reminder - we need to build a model to assign the title of item to proper root category)\n",
    "\n",
    "We will do it in few steps:\n",
    "1. Split the data to two parts - train data and test data - this is important to test the model on different data than we train\n",
    "2. Clean/normalize/build bag of word\n",
    "3. Prepare labels / category\n",
    "4. Build / train classifier\n",
    "5. Evaluate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train / test sets\n",
    "\n",
    "For split data we will use sklearn method [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split # import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_data['title'], df_data['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model:\n",
    "1. ~~Split the data to two parts - train data and test data - this is important to test the model on different data than we train~~\n",
    "2. Clean/normalize/build bag of word\n",
    "3. Prepare labels / category\n",
    "4. Build / train classifier\n",
    "5. Evaluate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean/normalize/build bag of word\n",
    "For this step we will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer will be responsible for creating a dictionary and word vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_vec = CountVectorizer(analyzer='word', # analyze only full word\n",
    "                                  lowercase=True, # lower case\n",
    "                                  ngram_range=(1, 1), # split by single word\n",
    "                                  stop_words=stop_words, # use our stop words\n",
    "                                  tokenizer=word_tokenize # use our nltk word_tokenizer\n",
    "                                 ) # create a simple count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because CountVectorizer is a model in sklearn we need to train this model - in sklearn to train model we are using `.fit`.\n",
    "\n",
    "Fit create just a vocabulary for bag of word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_vec.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained model - we need to transform data - create a vectors - for train and for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = bag_of_word_vec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = bag_of_word_vec.transform(X_test) #for test we have to create also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model:\n",
    "1. ~~Split the data to two parts - train data and test data - this is important to test the model on different data than we train~~\n",
    "2. ~~Clean/normalize/build bag of word~~\n",
    "3. Prepare labels / category\n",
    "4. Build / train classifier\n",
    "5. Evaluate classifier\n",
    "\n",
    "#### Preparing labels / category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target classes - categories are the text but for classifier we need change them to the numbers. It means that we need to transform text to simple number.\n",
    "\n",
    "**Example**\n",
    "1. Book should be change to 0\n",
    "2. TV -> 1\n",
    "etc. \n",
    "\n",
    "For that we need to import [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO base on code from CountVectorizer - create LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO train the label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO transform y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO transform y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model:\n",
    "1. ~~Split the data to two parts - train data and test data - this is important to test the model on different data than we train~~\n",
    "2. ~~Clean/normalize/build bag of word~~\n",
    "3. ~~Prepare labels / category~~\n",
    "4. Build / train classifier\n",
    "5. Evaluate classifier\n",
    "\n",
    "#### Build the models\n",
    "\n",
    "\n",
    "As a classifier we will use [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = #TODO predict the new value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 10 first prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show 10 first label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model:\n",
    "1. ~~Split the data to two parts - train data and test data - this is important to test the model on different data than we train~~\n",
    "2. ~~Clean/normalize/build bag of word~~\n",
    "3. ~~Prepare labels / category~~\n",
    "4. ~~Build / train classifier~~\n",
    "5. Evaluate classifier\n",
    "\n",
    "#### Evaluate the model\n",
    "\n",
    "\n",
    "In Sklearn we can create a classification report for each class for this we will use [classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report # metrics for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model:\n",
    "1. ~~Split the data to two parts - train data and test data - this is important to test the model on different data than we train~~\n",
    "2. ~~Clean/normalize/build bag of word~~\n",
    "3. ~~Prepare labels / category~~\n",
    "4. ~~Build / train classifier~~\n",
    "5. ~~Evaluate classifier~~\n",
    "\n",
    "FINISH - you create you first model for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve our model?\n",
    "\n",
    "1. Instead of single word vocabulary we can use multi word vocabulary it calls ngrams - how to do it? in Count vectorizer we need to change ```ngram_range = (1,1) ``` to ```ngram_range=(1,2)``` - it means use one and two words as a vocabulary - we can of course use only two-words or three-words\n",
    "2. Use different tokenization methods\n",
    "3. Use different stop words model\n",
    "4. Instead of simple bag of words we can use [TF-IDF]((TfidfVectorizer http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bonus.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TF-IDF\n",
    "\n",
    "TF-IDF is composed by two terms:\n",
    "1. TF - term frequency\n",
    "2. IDF - Inverse document frequency\n",
    "\n",
    "**How to compute **\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = Total number of documents / Number of documents with term t in it.\n",
    "\n",
    "TF-IDF = TF(t) * IDF(t)\n",
    "\n",
    "**Example**\n",
    "We have two sentences  ```The quick brown fox jumps over the lazy dog``` and  ```Never jump over the lazy dog quickly``` - for simplify we **do not** clean our text so `quick` is different than `quickly`\n",
    "\n",
    "word `quick` in first sentence appear one time so \n",
    "\n",
    "TF(quick) = 1 / 9 \n",
    "\n",
    "IDF(quick) = 2 / 1\n",
    "\n",
    "TF-IDF(quick) = (1 / 9) * 1 = 1/9\n",
    "\n",
    "Do the same with `dog` - but use the python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "dog_in_first_document = 1\n",
    "number_of_term_in_first_document = len(sentence.split())\n",
    "number_document_with_dog = 2\n",
    "number_of_document = 2\n",
    "\n",
    "TF = dog_in_first_document/number_of_term_in_first_document\n",
    "IDF = number_of_document / number_document_with_dog\n",
    "\n",
    "TF_IDF = TF*IDF\n",
    "print(TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do it use Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # import tfidf\n",
    "tf_idf = TfidfVectorizer(\n",
    "      analyzer='word', # analyze only full word\n",
    "      lowercase=True, # lower case\n",
    "      ngram_range=(1, 2), # split by single word\n",
    "      stop_words=stop_words, # use our stop words\n",
    "      tokenizer=word_tokenize # use our nltk word_tokenizer\n",
    ")\n",
    "\n",
    "#TODO - fit the Vectorizer (only on train), transform train/test set, build NaiveBayes classifier,\n",
    "# show the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PIPELINE](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "In sklearn we do not have to make each transform separately - we can use pipeline to simplify the model - below we have a simple example how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(TfidfVectorizer(ngram_range=(1,2)), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO show classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO compare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND THE BEST MODEL\n",
    "\n",
    "Many times during create new model we need to test many different parameters - like number of ngrams, learning rate and so on. \n",
    "\n",
    "Sklearn has methods for this - it is as simple as create new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = make_pipeline(TfidfVectorizer(ngram_range=(1,2)), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "              'tfidfvectorizer__use_idf': (True, False),\n",
    "              'tfidfvectorizer__norm': ('l1', 'l2'),\n",
    "              'multinomialnb__alpha': (1.0, 0.9, 0.8, 0.7),\n",
    " } #parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(clf_best, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO fit the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO predict the new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Compare this model with previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/links.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "2. [Natural Language Processing with Python](http://www.nltk.org/book/)\n",
    "3. [Deep Learning For NLP](https://github.com/andrewt3000/dl4nlp)\n",
    "4. [NLP step by step](https://www.tutorialspoint.com/artificial_intelligence/artificial_intelligence_natural_language_processing.htm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "name": "NLP introduction",
  "notebookId": 2230286274451564
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
